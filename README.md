# ai-patterns

[![npm version](https://img.shields.io/npm/v/ai-patterns.svg)](https://www.npmjs.com/package/ai-patterns)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue.svg)](https://www.typescriptlang.org/)

**Production-ready TypeScript patterns to build solid and robust AI applications.**

We provide developers with battle-tested tools for resilient AI workflows: retry logic, circuit breakers, rate limiting, human-in-the-loop escalation, and more ‚Äî all with complete type safety and composability. Inspired by Vercel AI SDK's developer experience.

## Features

‚ú® **15 Battle-Tested Patterns** - Retry, Circuit Breaker, Timeout, Rate Limiter, Fallback, Cache, Debounce, Throttle, Bulkhead, and more
üé® **Elegant Composition** - Functional middleware-based composition for complex workflows
üîí **Type-Safe** - Full TypeScript support with generics and strict mode
üß© **Composable** - Patterns work together seamlessly for robust workflows
üìä **Observable** - Built-in lifecycle callbacks for monitoring and debugging
ü™∂ **Lightweight** - Zero dependencies, minimal overhead
‚ö° **Production-Ready** - Build solid AI applications with confidence
üéØ **Developer-Friendly** - Inspired by Vercel AI SDK's excellent DX

## Installation

```bash
npm install ai-patterns
# or
yarn add ai-patterns
# or
pnpm add ai-patterns
```

## Quick Start

```typescript
import {
  compose,
  retryMiddleware,
  timeoutMiddleware,
  cacheMiddleware,
  BackoffStrategy
} from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

// Compose multiple patterns together for a robust AI workflow
const robustAI = compose([
  timeoutMiddleware({ duration: 10000 }),
  retryMiddleware({ maxAttempts: 3, backoffStrategy: BackoffStrategy.EXPONENTIAL }),
  cacheMiddleware({ ttl: 300000 }) // 5 min cache
]);

// Use it with any AI function
const result = await robustAI(
  async (input: { prompt: string }) => {
    const { text } = await generateText({
      model: openai('gpt-4-turbo'),
      prompt: input.prompt,
      maxRetries: 0 // We handle retries
    });
    return text;
  },
  { prompt: 'Explain quantum computing' }
);

console.log(result); // ‚úÖ Fully typed, cached, with retry & timeout
```

## Elegant Composition

Compose patterns together using functional composition, similar to mathematical function composition. Patterns are applied **right to left** (innermost first):

```typescript
import {
  compose,
  retryMiddleware,
  timeoutMiddleware,
  circuitBreakerMiddleware,
  fallbackMiddleware,
} from 'ai-patterns';

// Build a production-ready AI pipeline
const pipeline = compose([
  fallbackMiddleware({ fallback: async () => getCachedResponse() }),  // 4. Fallback (outermost)
  circuitBreakerMiddleware({ failureThreshold: 5 }),                  // 3. Circuit breaker
  timeoutMiddleware({ duration: 10000 }),                             // 2. Timeout
  retryMiddleware({ maxAttempts: 3 }),                                // 1. Retry (innermost)
]);

const result = await pipeline(
  async (input) => callAI(input),
  { prompt: 'Your prompt here' }
);
```

**Why composition?**
- üéØ **Declarative**: Describe *what* you want, not *how*
- üß© **Composable**: Mix and match patterns freely
- üìñ **Readable**: Clear, linear flow of transformations
- üîí **Type-Safe**: Full TypeScript inference through the chain

## Patterns

### Core Patterns

| Pattern | Description | Use Case | Docs |
|---------|-------------|----------|------|
| **[compose](#compose)** | Functional middleware-based composition | Complex AI pipelines | - |
| **[retry](#retry)** | Automatic retry with exponential backoff | Unstable APIs, network issues | [üìñ](./docs/patterns/retry.md) |
| **[timeout](#timeout)** | Time limits with AbortSignal support | Long-running operations | [üìñ](./docs/patterns/timeout.md) |
| **[fallback](#fallback)** | Execute alternatives on failure | Multi-provider failover | [üìñ](./docs/patterns/fallback.md) |
| **[defineCircuitBreaker](#definecircuitbreaker)** | Protect against failing services | External API calls | [üìñ](./docs/patterns/circuit-breaker.md) |
| **[defineRateLimiter](#defineratelimiter)** | Control request throughput | API rate limiting | [üìñ](./docs/patterns/rate-limiter.md) |

### Advanced Patterns

| Pattern | Description | Use Case | Docs |
|---------|-------------|----------|------|
| **[memoize](#memoize)** | Cache function results with TTL | Response caching | [üìñ](./docs/patterns/memoize.md) |
| **[defineDebounce](#debounce)** | Delay execution until silence period | User input handling | [üìñ](./docs/patterns/debounce.md) |
| **[defineThrottle](#throttle)** | Limit execution frequency | API call throttling | [üìñ](./docs/patterns/throttle.md) |
| **[defineBulkhead](#bulkhead)** | Isolate resources with concurrency limits | Resource isolation | [üìñ](./docs/patterns/bulkhead.md) |
| **[deadLetterQueue](#dead-letter-queue)** | Handle failed operations | Error recovery | [üìñ](./docs/patterns/dead-letter-queue.md) |

### Orchestration Patterns

| Pattern | Description | Use Case | Docs |
|---------|-------------|----------|------|
| **[fanOut](#fan-out)** | Parallel processing with concurrency control | Batch operations | [üìñ](./docs/patterns/fan-out.md) |
| **[saga](#saga)** | Distributed transactions with compensation | Multi-step workflows | [üìñ](./docs/patterns/saga.md) |
| **[conditionalBranch](#conditional-branch)** | Route based on conditions | Dynamic workflow routing | [üìñ](./docs/patterns/conditional-branch.md) |

### AI-Specific Patterns

| Pattern | Description | Use Case | Docs |
|---------|-------------|----------|------|
| **[humanInTheLoop](#human-in-the-loop)** | AI ‚Üí Human escalation | Content moderation | [üìñ](./docs/patterns/human-in-the-loop.md) |
| **[idempotency](#idempotency)** | Prevent duplicate operations | Payment processing | [üìñ](./docs/patterns/idempotency.md) |

---

## Pattern Examples

### compose

Compose multiple patterns together using functional middleware composition. Patterns are applied **right to left** (innermost first).

```typescript
import {
  compose,
  retryMiddleware,
  timeoutMiddleware,
  circuitBreakerMiddleware,
  cacheMiddleware,
  fallbackMiddleware,
} from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

// Build a production-ready AI pipeline
const robustAI = compose(
  [
    // Fallback if everything else fails (outermost)
    fallbackMiddleware({
      fallback: async () => "I'm currently unavailable. Please try again later."
    }),

    // Circuit breaker to prevent cascade failures
    circuitBreakerMiddleware({
      failureThreshold: 5,
      resetTimeout: 60000
    }),

    // Timeout after 10 seconds
    timeoutMiddleware({ duration: 10000 }),

    // Retry up to 3 times with exponential backoff
    retryMiddleware({
      maxAttempts: 3,
      backoffStrategy: BackoffStrategy.EXPONENTIAL
    }),

    // Cache responses for 5 minutes (innermost)
    cacheMiddleware({
      ttl: 5 * 60 * 1000,
      keyFn: (input) => input.prompt
    }),
  ],
  {
    onStart: () => console.log('Processing request...'),
    onComplete: (result, duration) => console.log(`‚úÖ Done in ${duration}ms`),
    onError: (error) => console.error('‚ùå Pipeline failed:', error)
  }
);

// Use the composed pipeline
const result = await robustAI(
  async (input: { prompt: string }) => {
    const { text } = await generateText({
      model: openai('gpt-4-turbo'),
      prompt: input.prompt,
      maxRetries: 0
    });
    return text;
  },
  { prompt: 'Explain quantum computing' }
);

console.log(result); // ‚úÖ Fully typed, cached, resilient response
```

**Available Middleware:**
- `retryMiddleware` - Retry with backoff strategies
- `timeoutMiddleware` - Timeout protection
- `fallbackMiddleware` - Execute fallback functions
- `circuitBreakerMiddleware` - Circuit breaker pattern
- `rateLimiterMiddleware` - Rate limiting
- `cacheMiddleware` - Response caching with TTL

**Composition Benefits:**
- üéØ Declarative and readable
- üîÑ Reusable pipelines
- üß© Mix and match freely
- üìä Lifecycle hooks (onStart, onComplete, onError)
- üîí Full type safety

### retry

Automatically retry failed operations with intelligent backoff strategies.

> **Note:** Vercel AI SDK's `generateText` and `generateObject` have built-in retry (`maxRetries: 2` by default). Use our `retry` pattern for:
> - **Advanced control**: Custom backoff strategies, detailed callbacks
> - **Cross-provider fallback**: Retry across OpenAI ‚Üí Claude ‚Üí Gemini
> - **More attempts**: Beyond Vercel's 2 retries
> - **Observability**: Detailed metrics (`attempts`, `totalDelay`)

```typescript
import { retry, BackoffStrategy } from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

interface AIResponse {
  text: string;
  provider: string;
}

// Advanced retry: Cross-provider fallback with exponential backoff
const result = await retry<AIResponse>({
  execute: async () => {
    try {
      // Try OpenAI first (disable its built-in retry)
      const response = await generateText({
        model: openai('gpt-4-turbo'),
        prompt: 'Explain quantum computing',
        maxRetries: 0 // ‚Üê Disable Vercel retry
      });
      return { text: response.text, provider: 'OpenAI' };
    } catch (error) {
      // Fallback to Claude
      const response = await generateText({
        model: anthropic('claude-3-5-sonnet-20241022'),
        prompt: 'Explain quantum computing',
        maxRetries: 0
      });
      return { text: response.text, provider: 'Claude' };
    }
  },
  maxAttempts: 5,
  initialDelay: 1000,
  backoffStrategy: BackoffStrategy.EXPONENTIAL,
  onRetry: (error, attempt, delay) => {
    console.log(`‚ö†Ô∏è  Attempt ${attempt} failed, retrying in ${delay}ms...`);
  }
});

console.log(`‚úÖ Success with ${result.value.provider}`); // ‚úÖ Fully typed
console.log(`üìä Took ${result.attempts} attempts`);      // Detailed metrics
console.log(`‚è±Ô∏è  Total delay: ${result.totalDelay}ms`);
```

**[üìñ Full retry documentation](./docs/patterns/retry.md)**

---

### timeout

Add time limits to async operations with AbortSignal support.

```typescript
import { timeout, TimeoutDurations } from 'ai-patterns';
import { anthropic } from '@ai-sdk/anthropic';
import { generateText } from 'ai';

interface AIResult {
  text: string;
  finishReason: string;
}

// Add timeout to long-running AI generation
const result = await timeout<AIResult>({
  execute: async () => {
    const response = await generateText({
      model: anthropic('claude-3-5-sonnet-20241022'),
      prompt: 'Write a detailed essay about the history of computing (2000 words)'
    });

    return {
      text: response.text,
      finishReason: response.finishReason
    };
  },
  timeoutMs: TimeoutDurations.LONG, // 30 seconds max
  onTimeout: () => {
    console.log('AI generation timed out');
  }
});

console.log(result.value.text.length); // ‚úÖ Fully typed
```

**[üìñ Full timeout documentation](./docs/patterns/timeout.md)**

---

### defineCircuitBreaker

Protect your system from cascading failures when calling external services.

```typescript
import { defineCircuitBreaker, CircuitState } from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

interface AIResponse {
  text: string;
  usage: { totalTokens: number };
}

// Protect against OpenAI outages with circuit breaker
const generateWithGPT4 = defineCircuitBreaker<AIResponse>({
  execute: async () => {
    const response = await generateText({
      model: openai('gpt-4-turbo'),
      prompt: 'Summarize the latest AI research trends'
    });

    return {
      text: response.text,
      usage: response.usage
    };
  },
  failureThreshold: 5,      // Open after 5 failures
  openDuration: 60000,      // Try again after 1 minute
  onStateChange: (oldState, newState) => {
    console.log(`OpenAI Circuit: ${oldState} ‚Üí ${newState}`);
    if (newState === CircuitState.OPEN) {
      // Fallback to Claude or cached response
      console.log('Switching to fallback AI provider');
    }
  }
});

const result = await generateWithGPT4(); // ‚úÖ Direct call (Vercel-style)
console.log(result.text);                 // ‚úÖ Fully typed
console.log(generateWithGPT4.getState()); // Check circuit state
```

**[üìñ Full circuit breaker documentation](./docs/patterns/circuit-breaker.md)**

---

### defineRateLimiter

Control request throughput with multiple strategies.

```typescript
import { defineRateLimiter, RateLimitStrategy } from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { generateObject } from 'ai';
import { z } from 'zod';

const recipeSchema = z.object({
  name: z.string(),
  ingredients: z.array(z.string()),
  steps: z.array(z.string())
});

type Recipe = z.infer<typeof recipeSchema>;

// Respect OpenAI rate limits (60 requests/minute on free tier)
const generateRecipe = defineRateLimiter<Recipe>({
  execute: async () => {
    const { object } = await generateObject({
      model: openai('gpt-4-turbo'),
      schema: recipeSchema,
      prompt: 'Generate a random recipe'
    });
    return object;
  },
  maxRequests: 60,
  windowMs: 60000, // 1 minute
  strategy: RateLimitStrategy.SLIDING_WINDOW
});

const result = await generateRecipe(); // ‚úÖ Direct call (Vercel-style)
console.log(result.value.name);              // ‚úÖ Fully typed
console.log(`${result.remaining}/60 requests remaining`);
console.log(generateRecipe.getRemaining()); // ‚úÖ Check remaining
```

**[üìñ Full rate limiter documentation](./docs/patterns/rate-limiter.md)**

---

### fanOut

Process multiple items in parallel with concurrency control.

```typescript
import { fanOut } from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { embed } from 'ai';

interface DocumentChunk {
  id: string;
  text: string;
}

interface EmbeddedChunk {
  id: string;
  embedding: number[];
}

const chunks: DocumentChunk[] = [
  { id: '1', text: 'Introduction to machine learning' },
  { id: '2', text: 'Deep learning fundamentals' },
  { id: '3', text: 'Natural language processing basics' },
  { id: '4', text: 'Computer vision applications' },
  { id: '5', text: 'Reinforcement learning concepts' }
];

// Generate embeddings in parallel for RAG system
const result = await fanOut<DocumentChunk, EmbeddedChunk>({
  items: chunks,
  execute: async (chunk) => {
    const { embedding } = await embed({
      model: openai.embedding('text-embedding-3-small'),
      value: chunk.text
    });

    return {
      id: chunk.id,
      embedding
    };
  },
  concurrency: 5, // Process 5 embeddings at once
  onProgress: (completed, total) => {
    console.log(`Embedded ${completed}/${total} chunks`);
  }
});

// Store in vector database (Pinecone, Weaviate, etc.)
result.results.forEach(chunk => {
  console.log(`Chunk ${chunk.id}: ${chunk.embedding.length} dimensions`); // ‚úÖ Fully typed
});
console.log(`Successfully embedded ${result.successCount} documents`);
```

**[üìñ Full fan-out documentation](./docs/patterns/fan-out.md)**

---

### saga

Implement distributed transactions with automatic compensation.

```typescript
import { executeSaga } from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { generateText, generateObject } from 'ai';
import { z } from 'zod';

interface AIAgentContext {
  userId: string;
  query: string;
  researchData?: string;
  analysis?: { summary: string; insights: string[] };
  reportId?: string;
}

// Multi-step AI workflow with automatic rollback
const result = await executeSaga<AIAgentContext>({
  context: {
    userId: 'user-123',
    query: 'Analyze recent AI developments in healthcare'
  },
  steps: [
    {
      name: 'Research Phase',
      execute: async (ctx) => {
        const { text } = await generateText({
          model: openai('gpt-4-turbo'),
          prompt: `Research this topic: ${ctx.query}`
        });
        ctx.researchData = text;
        return text;
      },
      compensate: async (ctx) => {
        console.log('Clearing research data');
        delete ctx.researchData;
      }
    },
    {
      name: 'Analysis Phase',
      execute: async (ctx) => {
        const { object } = await generateObject({
          model: openai('gpt-4-turbo'),
          schema: z.object({
            summary: z.string(),
            insights: z.array(z.string())
          }),
          prompt: `Analyze this research: ${ctx.researchData}`
        });
        ctx.analysis = object;
        return object;
      },
      compensate: async (ctx) => {
        console.log('Clearing analysis');
        delete ctx.analysis;
      }
    },
    {
      name: 'Save Report',
      execute: async (ctx) => {
        const reportId = await saveToDatabase({
          userId: ctx.userId,
          analysis: ctx.analysis
        });
        ctx.reportId = reportId;
        return reportId;
      },
      compensate: async (ctx) => {
        if (ctx.reportId) {
          await deleteFromDatabase(ctx.reportId);
        }
      }
    }
  ]
});

if (result.success) {
  console.log('AI workflow completed');
  console.log(result.context.analysis?.summary); // ‚úÖ Fully typed
} else {
  console.log(`Workflow failed - all steps rolled back`);
}
```

**[üìñ Full saga documentation](./docs/patterns/saga.md)**

---

### humanInTheLoop

Escalate AI decisions to humans when needed.

```typescript
import { humanInTheLoop, CommonEscalationRules } from 'ai-patterns';
import OpenAI from 'openai';

const openai = new OpenAI();

interface ModerationResult {
  decision: 'approved' | 'rejected' | 'review';
  confidence: number;
  flaggedCategories: string[];
}

const userContent = "User-generated content to moderate";

// AI moderation with human escalation for edge cases
const result = await humanInTheLoop<string, ModerationResult>({
  execute: async () => {
    const moderation = await openai.moderations.create({
      input: userContent
    });

    const flagged = moderation.results[0].flagged;
    const categories = Object.entries(moderation.results[0].categories)
      .filter(([_, value]) => value)
      .map(([key]) => key);

    // Calculate max confidence score
    const scores = Object.values(moderation.results[0].category_scores);
    const maxConfidence = Math.max(...scores);

    return {
      decision: flagged ? 'rejected' : 'approved',
      confidence: maxConfidence,
      flaggedCategories: categories
    };
  },
  input: userContent,
  escalationRules: [
    CommonEscalationRules.lowConfidence(0.8), // Escalate if confidence < 80%
    CommonEscalationRules.sensitiveKeywords(['self-harm', 'violence'])
  ],
  requestHumanReview: async (review) => {
    // Send to moderation queue
    return await moderationQueue.addReview(review);
  },
  onEscalate: (review) => {
    console.log(`Escalated to human: ${review.reason}`);
  }
});

console.log(result.value.decision);   // ‚úÖ Fully typed
console.log(result.value.confidence); // ‚úÖ Fully typed
```

**[üìñ Full human-in-the-loop documentation](./docs/patterns/human-in-the-loop.md)**

---

### idempotency

Ensure operations can be safely retried without duplicates.

```typescript
import { idempotent } from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

interface BlogPostResult {
  title: string;
  content: string;
  tokensUsed: number;
}

const topic = "The future of quantum computing";

// Prevent duplicate AI generations - save tokens and cost
const result = await idempotent<BlogPostResult>({
  execute: async () => {
    console.log('Generating new content (costs tokens)...');

    const { text, usage } = await generateText({
      model: openai('gpt-4-turbo'),
      prompt: `Write a detailed blog post about: ${topic}`
    });

    const [title, ...contentLines] = text.split('\n');

    return {
      title: title.replace(/^#\s*/, ''),
      content: contentLines.join('\n'),
      tokensUsed: usage.totalTokens
    };
  },
  key: `blog-post:${topic}`,
  ttl: 86400000, // Cache for 24 hours
  onCacheHit: (key) => {
    console.log(`Returning cached content - saved API call!`);
  }
});

console.log(result.title);       // ‚úÖ Fully typed
console.log(result.tokensUsed);  // ‚úÖ Fully typed

// Second call with same topic returns cached result (no API call)
const cached = await idempotent({...}); // ‚úÖ Instant, free
```

**[üìñ Full idempotency documentation](./docs/patterns/idempotency.md)**

---

## Pattern Composition

Combine patterns for powerful, production-ready AI workflows:

```typescript
import {
  retry,
  timeout,
  defineCircuitBreaker,
  defineRateLimiter,
  idempotent,
  BackoffStrategy,
  RateLimitStrategy,
  TimeoutDurations
} from 'ai-patterns';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

interface AIResponse {
  text: string;
  usage: { totalTokens: number };
}

// Production-ready AI agent: Circuit Breaker + Rate Limiter + Retry + Timeout + Idempotency
const productionAICall = defineCircuitBreaker<AIResponse>({
  execute: async () => {
    // Rate limiting (respect OpenAI limits)
    const limiter = defineRateLimiter<AIResponse>({
      execute: async () => {
        // Idempotency (cache results)
        return await idempotent<AIResponse>({
          execute: async () => {
            // Retry with exponential backoff
            return await retry<AIResponse>({
              execute: async () => {
                // Timeout protection
                return await timeout<AIResponse>({
                  execute: async () => {
                    const response = await generateText({
                      model: openai('gpt-4-turbo'),
                      prompt: 'Explain TypeScript generics'
                    });

                    return {
                      text: response.text,
                      usage: response.usage
                    };
                  },
                  timeoutMs: TimeoutDurations.LONG // 30s max
                });
              },
              maxAttempts: 3,
              backoffStrategy: BackoffStrategy.EXPONENTIAL
            });
          },
          key: 'ai-query:typescript-generics',
          ttl: 3600000 // 1 hour cache
        });
      },
      maxRequests: 60,
      windowMs: 60000, // 60 req/min
      strategy: RateLimitStrategy.SLIDING_WINDOW
    });

    return await limiter();
  },
  failureThreshold: 5, // Open circuit after 5 failures
  openDuration: 60000  // Try again after 1 minute
});

// Ultra-reliable AI call
const result = await productionAICall(); // ‚úÖ Vercel-style callable
console.log(result.text);                 // ‚úÖ Fully typed through ALL layers
console.log(productionAICall.getState()); // Monitor circuit state
```

**[üìñ Pattern Composition Guide](./docs/guides/composition.md)**

---

## Examples

### Basic Examples

Each pattern has a simple runnable example:

- [retry-simple.ts](./examples/basic/retry-simple.ts)
- [timeout-simple.ts](./examples/basic/timeout-simple.ts)
- [circuit-breaker-simple.ts](./examples/basic/circuit-breaker-simple.ts)
- [rate-limiter-simple.ts](./examples/basic/rate-limiter-simple.ts)
- [fan-out-simple.ts](./examples/basic/fan-out-simple.ts)
- [saga-simple.ts](./examples/basic/saga-simple.ts)
- [human-in-loop-simple.ts](./examples/basic/human-in-loop-simple.ts)
- [idempotency-simple.ts](./examples/basic/idempotency-simple.ts)

### Real-World Examples

Coming soon:

- **E-commerce** - Order processing with saga, retry, and idempotency
- **AI Agent** - Chatbot with human escalation and circuit breakers
- **Microservices** - API gateway with rate limiting and retries

---

## Documentation

### Pattern Documentation

- [Retry Pattern](./docs/patterns/retry.md)
- [Timeout Pattern](./docs/patterns/timeout.md)
- [Circuit Breaker Pattern](./docs/patterns/circuit-breaker.md)
- [Rate Limiter Pattern](./docs/patterns/rate-limiter.md)
- [Fan-Out Pattern](./docs/patterns/fan-out.md)
- [Saga Pattern](./docs/patterns/saga.md)
- [Human-in-the-Loop Pattern](./docs/patterns/human-in-the-loop.md)
- [Idempotency Pattern](./docs/patterns/idempotency.md)

### Guides

- [Getting Started](./docs/guides/getting-started.md)
- [Pattern Composition](./docs/guides/composition.md)
- [Error Handling](./docs/guides/error-handling.md)
- [Best Practices](./docs/guides/best-practices.md)

---

## API Reference

All patterns follow a consistent API design:

```typescript
const result = await pattern({
  execute: () => yourFunction(),
  // pattern-specific options...
});
```

See the [API Reference](./docs/api-reference.md) for complete details.

---

## Type Safety

Built with TypeScript strict mode for maximum type safety:

```typescript
// Full type inference with generics
interface User {
  id: string;
  name: string;
  email: string;
}

const result = await retry<User>({
  execute: async () => {
    return await fetchUser();
  }
});

// result.value is typed as User
const user: User = result.value;
console.log(user.email); // ‚úÖ Full autocomplete
```

---

## Contributing

Contributions are welcome! Please read our [Contributing Guide](./CONTRIBUTING.md).

---

## License

MIT ¬© [Serge KOKOUA](https://github.com/sergekokoua)

---

## Why ai-patterns?

Building production-grade AI applications requires more than just calling APIs. You need:

- ‚úÖ **Resilience** - Handle transient failures gracefully
- ‚úÖ **Reliability** - Ensure critical operations succeed
- ‚úÖ **Observability** - Monitor and debug AI workflows
- ‚úÖ **Cost Control** - Prevent duplicate operations, manage rate limits
- ‚úÖ **Human Oversight** - Escalate edge cases appropriately

**ai-patterns provides all of this out of the box**, so you can focus on building great AI features.

---

## Acknowledgments

Inspired by:
- [Vercel AI SDK](https://sdk.vercel.ai) - Developer experience
- [Polly](https://github.com/App-vNext/Polly) - Resilience patterns
- [ts-retry](https://github.com/wankdanker/ts-retry) - TypeScript patterns

---

**Built with ‚ù§Ô∏è by [Serge KOKOUA](https://github.com/sergekokoua)**

*Empowering developers to build solid and robust AI applications.*

---

## New Patterns Added

### Fallback
```typescript
import { fallback } from 'ai-patterns';

const result = await fallback({
  execute: async () => callPrimaryAPI(),
  fallback: async () => callBackupAPI()
});
```

### Debounce & Throttle
```typescript
import { defineDebounce, defineThrottle } from 'ai-patterns';

const save = defineDebounce({
  execute: async () => saveData(),
  wait: 500,
  maxWait: 2000
});

const track = defineThrottle({
  execute: async () => trackEvent(),
  interval: 1000
});
```

### Bulkhead
```typescript
import { defineBulkhead } from 'ai-patterns';

const protected = defineBulkhead({
  execute: async () => heavyTask(),
  maxConcurrent: 5,
  maxQueue: 100
});
```

### Memoize
```typescript
import { memoize } from 'ai-patterns';

const fetchUser = memoize({
  execute: async (id) => api.getUser(id),
  ttl: 60000,
  keyFn: (id) => `user:${id}`
});
```

### Dead Letter Queue
```typescript
import { deadLetterQueue } from 'ai-patterns';

await deadLetterQueue({
  execute: async (msg) => process(msg),
  maxRetries: 3,
  onDeadLetter: (item, errors) => sendToSupport(item)
}, message);
```

### Conditional Branch
```typescript
import { conditionalBranch } from 'ai-patterns';

await conditionalBranch({
  condition: (data) => data.amount > 1000,
  onTrue: async (data) => manualApproval(data),
  onFalse: async (data) => autoApprove(data)
}, { amount: 1500 });
```

